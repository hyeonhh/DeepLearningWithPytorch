
# Chapter 5 
## 온도 문제 - 선형 모델로 풀어보기 

### 배경과 목적
- 온도계를 구입했는데 온도계에 단위가 없다!!?
-  우리가 선호하는 단위를 사용하여 온도계를 읽고 일치하는 온도를 기록한 데이터셋을 만든 후
-  학습을 위한 모델을 하나 골라
-  오차가 충분히 낮아질 때까지 반복적으로 가중치를 조절해서,
-  마지막에는 우리가 이해하는 온도 단위로 새로운 눈금을 해석할 수 있도록 만들어보자

-  이 데이터를 학습해서 모델이 **단위 없는 온도계 눈금(t_u)를 섭씨 온도(t_c)로 바꾸는 변환 공식**을 스스로 학습하게 하는 게 목적
-  t_u : 단위 없는 온도계 눈금
-  t_c : 섭씨 온도

### 선형 모델으로 시도해보기 

- t_c = w * t_u + b
- 측정된 값과 예측값인 출력값 사이의 오차가 최대한 작아지게 만든다.
- 오차 측정을 어떻게 할 지 정의해야하는데 loss function을 만들 되, 오차가 높으면 출력값도 높아지도록 정의한다.
- 이렇게 손실 함수의 값이 최소인 지점에서 w와 b를 찾는 과정을 최적화 과정이라고 한다.

### 손실을 줄이기 위한 방안
  - 손실 함수 : training sample로부터 기대하는 "출력값"과 모델이 샘플에 대해 "실제 출력한 값" 사이의 차이를 계산한다.
  - 모델이 출력한 온도인 t_p와 실제 값과의 차이인 t_p - t_c
  - t_p가 t_c보다 작든 크든간에 항상 양수의 차이가 나오게 해서 t_p가 t_c로 맞춰가는 형태가 되도록 한다.
  - |t_p - t_c| 혹은 (t_p - t_c)^를 사용할 수 있음

### 파이토치로 문제 풀어보기
- 코드 참조

### 브로드 캐스팅
- pytorch에서 서로 다른 shape(차원)의 텐서끼리 연산할 때, 자동으로 작은 텐서의 차원을 늘려서 연산을 가능하게 해주는 기능
- 규칙
  1. 차원 수 맞추기 : 두 텐서의 차원이 다르면, 작은 쪽에 왼쪽부터 1을 추가해서 차원을 맞춘다.
  2. 각 차원별 크기 비교 : 뒤에서부터(오른쪽부터) 차원을 비교해서, 두 텐서의 해당 차원이 같거나 둘 중 하나가 1이면 연산이 가능하다.
  3. 크기가 1인 차원은 늘려서 사용 : 크기가 1인 차원은 자동으로 다른 텐서의 해당 차원 크기에 맞게 늘려서 연산한다.
  4. 둘다 1보다 크고 값이 다르면 에러 : 두 텐서의 해당 차원이 모두 1보다 크고 값이 다르면 , 브로드 캐스팅이 불가능, 에러 발생
- 예시 코드

  `x = torch.ones(())  # shape : () (스칼라)`

  `y = torch.ones(3,1) # shape : (3,1)`
  
  `z = torch.ones(1,3) # shape : (1,3)`
  
  `z = torch.ones(2,1,1) # shape : (2,1,1)`

  - x * y : 스칼라와 (3,1) 텐서의 곱
    - x : shape ()
    - y : shape (3,1)
    - 적용 규칙
      1. 규칙 1에 따라, 차원이 없는 x는 왼쪽에 1이 추가해서 (1,1)로 간주됨.
      2. 규칙 2에 따라, 각 차원에서 1인 쪽이 상대방의 크기에 맞게 늘어남 
    - 결과 : x가 y의 shape (3,1)로 브로드캐스팅
  
  - y * z : (3,1)와 (1,3) 텐서
    - y : shape (3,1)
    - z : shape (1,3)
    - 적용 규칙(규칙 2)
      -  두 텐서 모두 2차원이므로 각 차원별 비교
        - 첫 번째 차원 : 3 vs 1 -> 규칙 2에 따라 1인 쪽이 3으로 늘어남
        - 두 번째 차원 : 1 vs 3 -> 규칙 2에 따라 1인 쪽이 3으로 늘어남
    - 결과 : 두 텐서 모두 (3,3)로 브로드 캐스팅

  - y * z * a : (3,1) (1,3) (2,1,1) 텐서
    - 적용 규칙
      - 먼저 y와 z의 곱은 (3,3)
      - a * (y*z)할 때 a는 3차원, (y*z)는 2차원이므로, 규칙 1에 따라서 (y*z) 앞에 1을 추가해 (1,3,3)이 된다.
      - 각 차원별 비교(규칙2)
          - 첫 번째 차원 : 2 vs 1 -> 1인 쪽이 2로 늘어남
          - 두 번째 차원 : 1 vs 3 -> 1인 쪽이 3으로 늘어남
          - 세 번째 차원 : 1 vs 3 -> 1인 쪽이 3으로 늘어남

### Gradient descent : 손실 줄이기
- 경사하강 (gradient descent) 알고리즘을 사용해 파라미터 관점에서 손실함수를 최적화한다.
- 각 파라미터와 관련해 손실의 변화율을 계산해 손실이 줄어드는 방향으로 파라미터 값을 바꿔나간다.
- w = w - learning_rate * loss_rate_of_change_w
- w가 w1에서 w2까지 변했을 때 loss_fn이 어떻게 변화하는지 측정한다.
- 이 때 만약 w가 이동하는 거리를 극단적으로 줄인다면 ?
- w에 대해 loss_fn(손실함수)를 미분하는 것과 일치해진다! 그래서 미분을 사용하는 것이다.


### 순방향, 역방향 과정
- 순방향 : 입력 ~  출력(예측값-손실 계산하는 과정)
- 역방향 : 손실함수를 각 파라미터에 대해 미분하며 계산하는 과정, 파라미터 갱신 

### 파이토치의 자동 미분 
- 미분값을 구하기 위해서는 
  - `params = torch.tensor([1.0, 0.0],requires_grad= True)`
  - 모델 호출해 손실값을 구한 후 `loss = loss_fn(model(t_u,*params), t_c)`
  - loss 텐서에 대해 backward 호출 `loss.backward()`
- 이제 params의 grad는 params의 각 요소에 대한 손실값의 미분을 포함하고 있다.
- Pytorch는 연쇄적으로 연결된 함수들을 거쳐 손실에 대한 미분을 계산하고 값을 텐서의 grad 속성에 누적한다.
  - backward 호출은 미분을 말단 노드에 누적한다.
  - 따라서 backward 호출 전에 `params.grad.zero()`를 호출해야함

### 옵티마이저
`optimizer = torch.optim.SGD([params], lr = learning_rate)

### 훈련 , 검증 ,과적합

### 자동 미분 끄기
- pytorch는 연산 시 자동으로 계산 그래프(autograd graph)를 만든다.
- 이는 나중에 backward()를 호출해서 기울기를 계산할 때 필요하다.
- val_loss에 대해서는 backward를 호출하지 않는다.
- val_loss는 gradient 계산 필요없다.
- `with torch.no_grad():` 를 통해 자동 미분을 끌 수 있다.
- 예시 :

  `with torch.no_grad():
  val_t_p = model(val_t_u, *params)
  val_loss = loss_fn(val_t_p , val_t_c)
  asset val_loss.requires_grad == False`
`
